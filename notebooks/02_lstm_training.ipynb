{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model Training for Amazon Sentiment Analysis\n",
    "\n",
    "This notebook demonstrates the complete process of building, training, and evaluating an LSTM model for sentiment analysis of Amazon product reviews.\n",
    "\n",
    "## Objectives:\n",
    "1. Prepare data for LSTM training\n",
    "2. Build and configure LSTM architecture\n",
    "3. Train the model with proper validation\n",
    "4. Evaluate model performance\n",
    "5. Compare LSTM predictions with VADER sentiment\n",
    "6. Save trained model for production use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_processor import ReviewDataProcessor, get_data_stats\n",
    "from model_trainer import LSTMSentimentTrainer, evaluate_model\n",
    "from analysis_engine import SentimentAnalysisEngine\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Using GPU: {len(tf.config.experimental.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('../data/raw_reviews.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "stats = get_data_stats(df)\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Show rating distribution\n",
    "print(\"\\nRating Distribution:\")\n",
    "print(df['rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data processor\n",
    "processor = ReviewDataProcessor(max_vocab_size=10000, max_sequence_length=200)\n",
    "\n",
    "# Prepare data for training\n",
    "print(\"Preparing data for training...\")\n",
    "X_train, X_test, y_train, y_test = processor.prepare_data(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Vocabulary size: {len(processor.word_index)}\")\n",
    "print(f\"Max sequence length: {processor.max_sequence_length}\")\n",
    "\n",
    "# Show class distribution\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Class {int(label)} ({'Positive' if label == 1 else 'Negative'}): {count} ({count/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data preparation results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training set class distribution\n",
    "ax1.bar(['Negative', 'Positive'], counts, color=['red', 'green'], alpha=0.7)\n",
    "ax1.set_title('Training Set Class Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Number of Samples')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Sequence length distribution\n",
    "sequence_lengths = np.sum(X_train > 0, axis=1)  # Count non-zero (non-padded) tokens\n",
    "ax2.hist(sequence_lengths, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax2.set_title('Sequence Length Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Sequence Length')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axvline(sequence_lengths.mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {sequence_lengths.mean():.0f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Vocabulary coverage\n",
    "word_counts = list(processor.word_index.values())\n",
    "ax3.hist(word_counts[:1000], bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "ax3.set_title('Vocabulary Index Distribution (Top 1000)', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Word Index')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Sample of tokenized sequences\n",
    "sample_lengths = sequence_lengths[:100]\n",
    "ax4.plot(sample_lengths, 'b-', alpha=0.7)\n",
    "ax4.set_title('Sample Sequence Lengths (First 100)', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Sample Index')\n",
    "ax4.set_ylabel('Sequence Length')\n",
    "ax4.axhline(processor.max_sequence_length, color='red', linestyle='--', \n",
    "           label=f'Max Length: {processor.max_sequence_length}')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average sequence length: {sequence_lengths.mean():.1f} tokens\")\n",
    "print(f\"95th percentile: {np.percentile(sequence_lengths, 95):.0f} tokens\")\n",
    "print(f\"Sequences using full length: {np.sum(sequence_lengths == processor.max_sequence_length)} ({np.sum(sequence_lengths == processor.max_sequence_length)/len(sequence_lengths)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Model Architecture Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LSTM trainer\n",
    "vocab_size = len(processor.word_index)\n",
    "embedding_dim = 100\n",
    "lstm_units = 128\n",
    "max_sequence_length = processor.max_sequence_length\n",
    "\n",
    "trainer = LSTMSentimentTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    lstm_units=lstm_units,\n",
    "    max_sequence_length=max_sequence_length\n",
    ")\n",
    "\n",
    "print(\"LSTM Trainer initialized with:\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "print(f\"  LSTM units: {lstm_units}\")\n",
    "print(f\"  Max sequence length: {max_sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "print(\"Building LSTM model architecture...\")\n",
    "\n",
    "model = trainer.build_model(\n",
    "    dropout_rate=0.3,\n",
    "    recurrent_dropout=0.3,\n",
    "    l2_reg=0.01,\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "print(f\"\\nModel has {model.count_params():,} trainable parameters\")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nModel Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model architecture\n",
    "try:\n",
    "    plot_model(model, to_file='../models/model_architecture.png', \n",
    "               show_shapes=True, show_layer_names=True, dpi=150)\n",
    "    from IPython.display import Image\n",
    "    display(Image('../models/model_architecture.png'))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display model plot: {e}\")\n",
    "    print(\"Model architecture visualization saved to ../models/model_architecture.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "print(\"This may take several minutes depending on your hardware.\")\n",
    "\n",
    "# Training parameters\n",
    "epochs = 25  # Reduced for demo, increase to 50+ for production\n",
    "batch_size = 32\n",
    "validation_split = 0.1\n",
    "\n",
    "# Train the model\n",
    "history = trainer.train_model(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=validation_split\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation accuracy: {max(history['val_accuracy']):.4f}\")\n",
    "print(f\"Final test accuracy: {history['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training History Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Training & Validation Loss\n",
    "epochs_range = range(1, len(history['loss']) + 1)\n",
    "ax1.plot(epochs_range, history['loss'], 'b-', label='Training Loss', alpha=0.8)\n",
    "ax1.plot(epochs_range, history['val_loss'], 'r-', label='Validation Loss', alpha=0.8)\n",
    "ax1.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Training & Validation Accuracy\n",
    "ax2.plot(epochs_range, history['accuracy'], 'b-', label='Training Accuracy', alpha=0.8)\n",
    "ax2.plot(epochs_range, history['val_accuracy'], 'r-', label='Validation Accuracy', alpha=0.8)\n",
    "ax2.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision and Recall\n",
    "ax3.plot(epochs_range, history['precision'], 'g-', label='Training Precision', alpha=0.8)\n",
    "ax3.plot(epochs_range, history['val_precision'], 'orange', label='Validation Precision', alpha=0.8)\n",
    "ax3.plot(epochs_range, history['recall'], 'purple', label='Training Recall', alpha=0.8)\n",
    "ax3.plot(epochs_range, history['val_recall'], 'brown', label='Validation Recall', alpha=0.8)\n",
    "ax3.set_title('Precision and Recall', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate (if available)\n",
    "if 'lr' in history:\n",
    "    ax4.plot(epochs_range, history['lr'], 'g-', alpha=0.8)\n",
    "    ax4.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Learning Rate')\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "else:\n",
    "    # Show final metrics instead\n",
    "    metrics = ['Test Loss', 'Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1']\n",
    "    values = [history['test_loss'], history['test_accuracy'], \n",
    "              history['test_precision'], history['test_recall'], history['test_f1']]\n",
    "    \n",
    "    bars = ax4.bar(metrics, values, color=['red', 'green', 'blue', 'orange', 'purple'], alpha=0.7)\n",
    "    ax4.set_title('Final Test Metrics', fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('Score')\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "print(f\"  Test Loss: {history['test_loss']:.4f}\")\n",
    "print(f\"  Test Accuracy: {history['test_accuracy']:.4f}\")\n",
    "print(f\"  Test Precision: {history['test_precision']:.4f}\")\n",
    "print(f\"  Test Recall: {history['test_recall']:.4f}\")\n",
    "print(f\"  Test F1-Score: {history['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "y_pred_prob = trainer.predict(X_test)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "metrics = evaluate_model(trainer.model, X_test, y_test)\n",
    "\n",
    "print(\"\\nComprehensive Model Evaluation:\")\n",
    "print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives: {metrics['true_negatives']}\")\n",
    "print(f\"  False Positives: {metrics['false_positives']}\")\n",
    "print(f\"  False Negatives: {metrics['false_negatives']}\")\n",
    "print(f\"  True Positives: {metrics['true_positives']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model evaluation\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = np.array(metrics['confusion_matrix'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "ax1.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "ax1.set_ylabel('True Label')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.8)\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction probability distribution\n",
    "ax3.hist(y_pred_prob[y_test == 0], bins=50, alpha=0.5, label='Negative', color='red', density=True)\n",
    "ax3.hist(y_pred_prob[y_test == 1], bins=50, alpha=0.5, label='Positive', color='green', density=True)\n",
    "ax3.axvline(0.5, color='black', linestyle='--', alpha=0.8, label='Decision Threshold')\n",
    "ax3.set_title('Prediction Probability Distribution', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Prediction Probability')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Model confidence analysis\n",
    "confidence = np.abs(y_pred_prob - 0.5) * 2  # Scale to 0-1\n",
    "correct_predictions = (y_pred == y_test)\n",
    "\n",
    "ax4.scatter(confidence[correct_predictions], y_pred_prob[correct_predictions], \n",
    "           alpha=0.6, color='green', label='Correct', s=20)\n",
    "ax4.scatter(confidence[~correct_predictions], y_pred_prob[~correct_predictions], \n",
    "           alpha=0.6, color='red', label='Incorrect', s=20)\n",
    "ax4.set_xlabel('Prediction Confidence')\n",
    "ax4.set_ylabel('Prediction Probability')\n",
    "ax4.set_title('Prediction Confidence vs Accuracy', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Average Prediction Confidence: {confidence.mean():.4f}\")\n",
    "print(f\"Confidence for Correct Predictions: {confidence[correct_predictions].mean():.4f}\")\n",
    "print(f\"Confidence for Incorrect Predictions: {confidence[~correct_predictions].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Classification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification report\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(\"=\" * 50)\n",
    "target_names = ['Negative', 'Positive']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Analyze prediction errors\n",
    "print(\"\\nError Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# False Positives (predicted positive, actually negative)\n",
    "false_positives = (y_pred == 1) & (y_test == 0)\n",
    "print(f\"False Positives: {np.sum(false_positives)} ({np.sum(false_positives)/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# False Negatives (predicted negative, actually positive)  \n",
    "false_negatives = (y_pred == 0) & (y_test == 1)\n",
    "print(f\"False Negatives: {np.sum(false_negatives)} ({np.sum(false_negatives)/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# High confidence errors\n",
    "high_conf_errors = ~correct_predictions & (confidence > 0.7)\n",
    "print(f\"High Confidence Errors: {np.sum(high_conf_errors)} ({np.sum(high_conf_errors)/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Low confidence correct predictions\n",
    "low_conf_correct = correct_predictions & (confidence < 0.3)\n",
    "print(f\"Low Confidence Correct: {np.sum(low_conf_correct)} ({np.sum(low_conf_correct)/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and tokenizer\n",
    "model_path = '../models/lstm_sentiment_model.h5'\n",
    "tokenizer_path = '../models/tokenizer.pkl'\n",
    "\n",
    "print(\"Saving trained model and tokenizer...\")\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(model_path)\n",
    "\n",
    "# Save tokenizer\n",
    "processor.save_tokenizer(tokenizer_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Tokenizer saved to: {tokenizer_path}\")\n",
    "print(\"\\nModel and tokenizer are ready for production use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Testing with Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with sample reviews\n",
    "print(\"Testing model with sample reviews:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample reviews for testing\n",
    "test_reviews = [\n",
    "    \"This product is absolutely amazing! Great quality and fast shipping. Highly recommend!\",\n",
    "    \"Terrible product. Broke after one day. Complete waste of money. Very disappointed.\",\n",
    "    \"It's okay. Does what it's supposed to do but nothing special. Average quality.\",\n",
    "    \"Outstanding service and excellent product quality. Will definitely buy again!\",\n",
    "    \"Poor quality control and awful customer service. Avoid at all costs!\"\n",
    "]\n",
    "\n",
    "expected_sentiments = ['Positive', 'Negative', 'Neutral', 'Positive', 'Negative']\n",
    "\n",
    "for i, (review, expected) in enumerate(zip(test_reviews, expected_sentiments)):\n",
    "    # Preprocess the review\n",
    "    X_sample = processor.preprocess_single_text(review)\n",
    "    \n",
    "    # Get prediction\n",
    "    prob = trainer.predict(X_sample)[0]\n",
    "    prediction = 'Positive' if prob >= 0.5 else 'Negative'\n",
    "    confidence = abs(prob - 0.5) * 2\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Text: {review}\")\n",
    "    print(f\"Expected: {expected}\")\n",
    "    print(f\"Predicted: {prediction} (probability: {prob:.3f}, confidence: {confidence:.3f})\")\n",
    "    print(f\"Correct: {'âœ“' if prediction == expected or (expected == 'Neutral' and 0.3 <= prob <= 0.7) else 'âœ—'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare LSTM with VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analysis engine with our trained model\n",
    "print(\"Comparing LSTM with VADER sentiment analysis...\")\n",
    "\n",
    "try:\n",
    "    engine = SentimentAnalysisEngine(model_path, tokenizer_path)\n",
    "    print(\"Analysis engine loaded successfully!\")\n",
    "    \n",
    "    # Test comparison with sample reviews\n",
    "    print(\"\\nLSTM vs VADER Comparison:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, review in enumerate(test_reviews[:3]):\n",
    "        # Get comprehensive analysis\n",
    "        analysis = engine.comprehensive_analysis(review)\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Text: {review[:80]}...\" if len(review) > 80 else f\"Text: {review}\")\n",
    "        print(f\"VADER: {analysis['vader_sentiment']} (score: {analysis['vader_compound']:.3f})\")\n",
    "        print(f\"LSTM: {analysis['lstm_sentiment']} (prob: {analysis['lstm_probability']:.3f})\")\n",
    "        print(f\"Agreement: {'âœ“' if analysis['sentiment_agreement'] else 'âœ—'}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not load analysis engine: {e}\")\n",
    "    print(\"This is normal if running in a limited environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"LSTM MODEL TRAINING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nðŸ“Š DATASET:\")\n",
    "print(f\"  â€¢ Total samples: {len(X_train) + len(X_test):,}\")\n",
    "print(f\"  â€¢ Training samples: {len(X_train):,}\")\n",
    "print(f\"  â€¢ Test samples: {len(X_test):,}\")\n",
    "print(f\"  â€¢ Vocabulary size: {vocab_size:,} words\")\n",
    "print(f\"  â€¢ Average sequence length: {sequence_lengths.mean():.1f} tokens\")\n",
    "\n",
    "print(f\"\\nðŸ—ï¸ MODEL ARCHITECTURE:\")\n",
    "print(f\"  â€¢ Model type: Bidirectional LSTM\")\n",
    "print(f\"  â€¢ Embedding dimension: {embedding_dim}\")\n",
    "print(f\"  â€¢ LSTM units: {lstm_units}\")\n",
    "print(f\"  â€¢ Total parameters: {model.count_params():,}\")\n",
    "print(f\"  â€¢ Dropout rate: 30%\")\n",
    "print(f\"  â€¢ L2 regularization: 0.01\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ TRAINING RESULTS:\")\n",
    "print(f\"  â€¢ Training epochs: {len(history['loss'])}\")\n",
    "print(f\"  â€¢ Final training accuracy: {history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  â€¢ Best validation accuracy: {max(history['val_accuracy']):.4f}\")\n",
    "print(f\"  â€¢ Final test accuracy: {history['test_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ MODEL PERFORMANCE:\")\n",
    "print(f\"  â€¢ Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"  â€¢ Test Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"  â€¢ Test Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"  â€¢ Test F1-Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"  â€¢ ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ SAVED ARTIFACTS:\")\n",
    "print(f\"  â€¢ Model: {model_path}\")\n",
    "print(f\"  â€¢ Tokenizer: {tokenizer_path}\")\n",
    "print(f\"  â€¢ Architecture diagram: ../models/model_architecture.png\")\n",
    "\n",
    "print(f\"\\nðŸš€ NEXT STEPS:\")\n",
    "print(f\"  1. Use the trained model for production sentiment analysis\")\n",
    "print(f\"  2. Run comprehensive analysis with: python main.py analyze\")\n",
    "print(f\"  3. Compare LSTM predictions with VADER sentiment\")\n",
    "print(f\"  4. Generate business insights from sentiment-rating discrepancies\")\n",
    "print(f\"  5. Consider model improvements: hyperparameter tuning, ensemble methods\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"ðŸŽ‰ MODEL TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"The model is ready for production use and analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model validation\n",
    "print(\"\\nFinal Model Validation:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check if model files exist and are accessible\n",
    "import os\n",
    "\n",
    "files_to_check = [model_path, tokenizer_path]\n",
    "for file_path in files_to_check:\n",
    "    if os.path.exists(file_path):\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"âœ“ {file_path} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"âœ— {file_path} - Not found\")\n",
    "\n",
    "# Performance validation\n",
    "if metrics['accuracy'] > 0.85:\n",
    "    print(f\"\\nâœ“ Model performance is excellent (Accuracy: {metrics['accuracy']:.3f})\")\n",
    "elif metrics['accuracy'] > 0.80:\n",
    "    print(f\"\\nâœ“ Model performance is good (Accuracy: {metrics['accuracy']:.3f})\")\n",
    "else:\n",
    "    print(f\"\\nâš  Model performance could be improved (Accuracy: {metrics['accuracy']:.3f})\")\n",
    "\n",
    "print(f\"\\nðŸ”„ Ready for deployment and business analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}