{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Reviews Data Exploration and NLTK Analysis\n",
    "\n",
    "This notebook provides comprehensive exploratory data analysis (EDA) of the Amazon product reviews dataset and demonstrates NLTK-based sentiment analysis techniques.\n",
    "\n",
    "## Objectives:\n",
    "1. Understand dataset structure and characteristics\n",
    "2. Explore rating distributions and patterns\n",
    "3. Analyze text characteristics and quality\n",
    "4. Implement NLTK-based sentiment analysis\n",
    "5. Identify potential data quality issues\n",
    "6. Generate insights for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data_processor import ReviewDataProcessor, get_data_stats\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/raw_reviews.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comprehensive dataset statistics\n",
    "stats = get_data_stats(df)\n",
    "print(\"Comprehensive Dataset Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rating Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating distribution\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Rating count distribution\n",
    "rating_counts = df['rating'].value_counts().sort_index()\n",
    "ax1.bar(rating_counts.index, rating_counts.values, color='skyblue', alpha=0.8)\n",
    "ax1.set_title('Distribution of Star Ratings', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Star Rating')\n",
    "ax1.set_ylabel('Number of Reviews')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Rating percentage pie chart\n",
    "rating_pct = df['rating'].value_counts().sort_index()\n",
    "ax2.pie(rating_pct.values, labels=[f'{i} Star' for i in rating_pct.index], \n",
    "        autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Rating Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Helpful votes by rating\n",
    "df.boxplot(column='helpful_votes', by='rating', ax=ax3)\n",
    "ax3.set_title('Helpful Votes by Rating', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Star Rating')\n",
    "ax3.set_ylabel('Helpful Votes')\n",
    "\n",
    "# Verified purchase by rating\n",
    "verified_by_rating = df.groupby('rating')['verified_purchase'].mean()\n",
    "ax4.bar(verified_by_rating.index, verified_by_rating.values, color='lightgreen', alpha=0.8)\n",
    "ax4.set_title('Verified Purchase Rate by Rating', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Star Rating')\n",
    "ax4.set_ylabel('Verified Purchase Rate')\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Product and User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Reviews per product\n",
    "product_counts = df['product_name'].value_counts()\n",
    "ax1.barh(range(len(product_counts)), product_counts.values)\n",
    "ax1.set_yticks(range(len(product_counts)))\n",
    "ax1.set_yticklabels([name.replace(' ', '\\n') for name in product_counts.index], fontsize=10)\n",
    "ax1.set_title('Number of Reviews per Product', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Number of Reviews')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Average rating per product\n",
    "avg_rating_by_product = df.groupby('product_name')['rating'].mean().sort_values(ascending=True)\n",
    "ax2.barh(range(len(avg_rating_by_product)), avg_rating_by_product.values, color='orange', alpha=0.8)\n",
    "ax2.set_yticks(range(len(avg_rating_by_product)))\n",
    "ax2.set_yticklabels([name.replace(' ', '\\n') for name in avg_rating_by_product.index], fontsize=10)\n",
    "ax2.set_title('Average Rating per Product', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Average Rating')\n",
    "ax2.set_xlim(1, 5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Product Statistics:\")\n",
    "print(f\"Total unique products: {df['product_id'].nunique()}\")\n",
    "print(f\"Most reviewed product: {product_counts.index[0]} ({product_counts.iloc[0]} reviews)\")\n",
    "print(f\"Highest rated product: {avg_rating_by_product.index[-1]} ({avg_rating_by_product.iloc[-1]:.2f} stars)\")\n",
    "print(f\"Lowest rated product: {avg_rating_by_product.index[0]} ({avg_rating_by_product.iloc[0]:.2f} stars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Analysis and Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "df['text_length'] = df['review_text'].str.len()\n",
    "df['word_count'] = df['review_text'].str.split().str.len()\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Text length distribution\n",
    "ax1.hist(df['text_length'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('Distribution of Review Text Length', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Text Length (characters)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.axvline(df['text_length'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"text_length\"].mean():.0f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Word count distribution\n",
    "ax2.hist(df['word_count'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "ax2.set_title('Distribution of Word Count', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Word Count')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axvline(df['word_count'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"word_count\"].mean():.0f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Text length by rating\n",
    "df.boxplot(column='text_length', by='rating', ax=ax3)\n",
    "ax3.set_title('Text Length by Rating', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Star Rating')\n",
    "ax3.set_ylabel('Text Length (characters)')\n",
    "\n",
    "# Word count by rating\n",
    "df.boxplot(column='word_count', by='rating', ax=ax4)\n",
    "ax4.set_title('Word Count by Rating', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Star Rating')\n",
    "ax4.set_ylabel('Word Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text statistics\n",
    "print(\"Text Statistics:\")\n",
    "print(f\"Average text length: {df['text_length'].mean():.0f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.0f} words\")\n",
    "print(f\"Shortest review: {df['text_length'].min()} characters\")\n",
    "print(f\"Longest review: {df['text_length'].max()} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NLTK Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply VADER sentiment analysis\n",
    "print(\"Applying VADER sentiment analysis...\")\n",
    "sentiment_scores = df['review_text'].apply(lambda x: sia.polarity_scores(x))\n",
    "\n",
    "# Extract sentiment components\n",
    "df['vader_compound'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "df['vader_positive'] = sentiment_scores.apply(lambda x: x['pos'])\n",
    "df['vader_negative'] = sentiment_scores.apply(lambda x: x['neg'])\n",
    "df['vader_neutral'] = sentiment_scores.apply(lambda x: x['neu'])\n",
    "\n",
    "# Create sentiment labels\n",
    "df['vader_sentiment'] = df['vader_compound'].apply(\n",
    "    lambda x: 'positive' if x >= 0.05 else ('negative' if x <= -0.05 else 'neutral')\n",
    ")\n",
    "\n",
    "print(\"VADER sentiment analysis completed!\")\n",
    "print(f\"\\nVADER Sentiment Distribution:\")\n",
    "print(df['vader_sentiment'].value_counts())\n",
    "print(f\"\\nPercentages:\")\n",
    "print(df['vader_sentiment'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize VADER sentiment analysis results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# VADER compound score distribution\n",
    "ax1.hist(df['vader_compound'], bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "ax1.set_title('Distribution of VADER Compound Scores', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Compound Score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.axvline(0, color='red', linestyle='--', label='Neutral (0)')\n",
    "ax1.axvline(0.05, color='green', linestyle='--', label='Positive threshold')\n",
    "ax1.axvline(-0.05, color='orange', linestyle='--', label='Negative threshold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# VADER sentiment by star rating\n",
    "sentiment_rating_crosstab = pd.crosstab(df['rating'], df['vader_sentiment'], normalize='index') * 100\n",
    "sentiment_rating_crosstab.plot(kind='bar', ax=ax2, stacked=True)\n",
    "ax2.set_title('VADER Sentiment Distribution by Star Rating', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Star Rating')\n",
    "ax2.set_ylabel('Percentage')\n",
    "ax2.legend(title='VADER Sentiment')\n",
    "ax2.tick_params(axis='x', rotation=0)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Average VADER compound score by rating\n",
    "avg_compound_by_rating = df.groupby('rating')['vader_compound'].mean()\n",
    "ax3.bar(avg_compound_by_rating.index, avg_compound_by_rating.values, color='teal', alpha=0.8)\n",
    "ax3.set_title('Average VADER Compound Score by Rating', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Star Rating')\n",
    "ax3.set_ylabel('Average Compound Score')\n",
    "ax3.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# VADER sentiment components heatmap\n",
    "sentiment_components = df[['vader_positive', 'vader_negative', 'vader_neutral']].corr()\n",
    "sns.heatmap(sentiment_components, annot=True, cmap='coolwarm', center=0, ax=ax4)\n",
    "ax4.set_title('VADER Sentiment Components Correlation', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rating vs VADER Sentiment Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between star ratings and VADER sentiment\n",
    "correlation = df['rating'].corr(df['vader_compound'])\n",
    "print(f\"Correlation between Star Rating and VADER Compound Score: {correlation:.3f}\")\n",
    "\n",
    "# Scatter plot of rating vs VADER compound score\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot with trend line\n",
    "ax1.scatter(df['rating'], df['vader_compound'], alpha=0.6, s=20)\n",
    "z = np.polyfit(df['rating'], df['vader_compound'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax1.plot(df['rating'], p(df['rating']), \"r--\", alpha=0.8, linewidth=2)\n",
    "ax1.set_title(f'Star Rating vs VADER Compound Score\\n(Correlation: {correlation:.3f})', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Star Rating')\n",
    "ax1.set_ylabel('VADER Compound Score')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot of VADER compound scores by rating\n",
    "df.boxplot(column='vader_compound', by='rating', ax=ax2)\n",
    "ax2.set_title('VADER Compound Score Distribution by Rating', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Star Rating')\n",
    "ax2.set_ylabel('VADER Compound Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify potential mismatches\n",
    "print(\"\\nPotential Rating-Sentiment Mismatches:\")\n",
    "\n",
    "# High ratings with negative sentiment\n",
    "high_rating_negative = df[(df['rating'] >= 4) & (df['vader_compound'] < -0.1)]\n",
    "print(f\"High ratings (4-5 stars) with negative sentiment: {len(high_rating_negative)} reviews\")\n",
    "\n",
    "# Low ratings with positive sentiment  \n",
    "low_rating_positive = df[(df['rating'] <= 2) & (df['vader_compound'] > 0.1)]\n",
    "print(f\"Low ratings (1-2 stars) with positive sentiment: {len(low_rating_positive)} reviews\")\n",
    "\n",
    "if len(high_rating_negative) > 0:\n",
    "    print(\"\\nSample high-rated reviews with negative sentiment:\")\n",
    "    for i, (_, row) in enumerate(high_rating_negative.head(3).iterrows()):\n",
    "        print(f\"{i+1}. Rating: {row['rating']}, VADER: {row['vader_compound']:.3f}\")\n",
    "        print(f\"   Text: {row['review_text'][:100]}...\\n\")\n",
    "\n",
    "if len(low_rating_positive) > 0:\n",
    "    print(\"Sample low-rated reviews with positive sentiment:\")\n",
    "    for i, (_, row) in enumerate(low_rating_positive.head(3).iterrows()):\n",
    "        print(f\"{i+1}. Rating: {row['rating']}, VADER: {row['vader_compound']:.3f}\")\n",
    "        print(f\"   Text: {row['review_text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Processing and Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data processor for text cleaning\n",
    "processor = ReviewDataProcessor()\n",
    "\n",
    "# Clean text and analyze word frequencies\n",
    "print(\"Cleaning text and analyzing word frequencies...\")\n",
    "df['cleaned_text'] = df['review_text'].apply(processor.clean_text)\n",
    "\n",
    "# Get all words from cleaned text\n",
    "all_words = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for text in df['cleaned_text']:\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [word for word in words if word.isalpha() and word not in stop_words and len(word) > 2]\n",
    "    all_words.extend(filtered_words)\n",
    "\n",
    "# Word frequency analysis\n",
    "word_freq = FreqDist(all_words)\n",
    "most_common = word_freq.most_common(20)\n",
    "\n",
    "print(f\"\\nTotal unique words: {len(word_freq)}\")\n",
    "print(f\"Total words (after filtering): {len(all_words)}\")\n",
    "print(f\"\\nTop 20 most common words:\")\n",
    "for word, count in most_common:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word frequencies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Most common words bar chart\n",
    "words, counts = zip(*most_common)\n",
    "ax1.barh(range(len(words)), counts)\n",
    "ax1.set_yticks(range(len(words)))\n",
    "ax1.set_yticklabels(words)\n",
    "ax1.set_title('Top 20 Most Common Words', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Frequency')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Word frequency distribution (Zipf's law)\n",
    "ranks = range(1, min(1000, len(word_freq)) + 1)\n",
    "frequencies = [word_freq.most_common(1000)[i][1] for i in range(min(1000, len(word_freq)))]\n",
    "\n",
    "ax2.loglog(ranks, frequencies, 'b-', alpha=0.8)\n",
    "ax2.set_title('Word Frequency Distribution (Log-Log Scale)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Rank')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Word Frequency by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze word frequencies by sentiment\n",
    "def get_words_by_sentiment(df, sentiment, top_n=15):\n",
    "    \"\"\"Get most frequent words for a specific sentiment\"\"\"\n",
    "    sentiment_texts = df[df['vader_sentiment'] == sentiment]['cleaned_text']\n",
    "    words = []\n",
    "    \n",
    "    for text in sentiment_texts:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) > 2]\n",
    "        words.extend(filtered_tokens)\n",
    "    \n",
    "    return FreqDist(words).most_common(top_n)\n",
    "\n",
    "# Get word frequencies by sentiment\n",
    "positive_words = get_words_by_sentiment(df, 'positive')\n",
    "negative_words = get_words_by_sentiment(df, 'negative')\n",
    "neutral_words = get_words_by_sentiment(df, 'neutral')\n",
    "\n",
    "# Visualize word frequencies by sentiment\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))\n",
    "\n",
    "# Positive sentiment words\n",
    "pos_words, pos_counts = zip(*positive_words)\n",
    "ax1.barh(range(len(pos_words)), pos_counts, color='green', alpha=0.7)\n",
    "ax1.set_yticks(range(len(pos_words)))\n",
    "ax1.set_yticklabels(pos_words)\n",
    "ax1.set_title('Most Common Words in Positive Reviews', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Frequency')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Negative sentiment words\n",
    "neg_words, neg_counts = zip(*negative_words)\n",
    "ax2.barh(range(len(neg_words)), neg_counts, color='red', alpha=0.7)\n",
    "ax2.set_yticks(range(len(neg_words)))\n",
    "ax2.set_yticklabels(neg_words)\n",
    "ax2.set_title('Most Common Words in Negative Reviews', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Frequency')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Neutral sentiment words\n",
    "neu_words, neu_counts = zip(*neutral_words)\n",
    "ax3.barh(range(len(neu_words)), neu_counts, color='gray', alpha=0.7)\n",
    "ax3.set_yticks(range(len(neu_words)))\n",
    "ax3.set_yticklabels(neu_words)\n",
    "ax3.set_title('Most Common Words in Neutral Reviews', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Frequency')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Word Analysis by Sentiment:\")\n",
    "print(f\"\\nPositive reviews - Top words: {', '.join([word for word, _ in positive_words[:5]])}\")\n",
    "print(f\"Negative reviews - Top words: {', '.join([word for word, _ in negative_words[:5]])}\")\n",
    "print(f\"Neutral reviews - Top words: {', '.join([word for word, _ in neutral_words[:5]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "print(\"Data Quality Assessment:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_reviews = df.duplicated(subset=['review_text']).sum()\n",
    "print(f\"Duplicate reviews (by text): {duplicate_reviews}\")\n",
    "\n",
    "# Check for very short reviews\n",
    "very_short = (df['word_count'] < 5).sum()\n",
    "print(f\"Very short reviews (<5 words): {very_short}\")\n",
    "\n",
    "# Check for very long reviews\n",
    "very_long = (df['word_count'] > 200).sum()\n",
    "print(f\"Very long reviews (>200 words): {very_long}\")\n",
    "\n",
    "# Check for missing or empty text\n",
    "empty_text = df['review_text'].isna().sum() + (df['review_text'] == '').sum()\n",
    "print(f\"Empty review texts: {empty_text}\")\n",
    "\n",
    "# Check rating distribution balance\n",
    "rating_balance = df['rating'].value_counts(normalize=True)\n",
    "print(f\"\\nRating distribution balance:\")\n",
    "for rating, pct in rating_balance.sort_index().items():\n",
    "    print(f\"  {rating} stars: {pct:.1%}\")\n",
    "\n",
    "# Check for potential spam indicators\n",
    "print(f\"\\nPotential quality issues:\")\n",
    "print(f\"Reviews with excessive exclamation marks: {df['review_text'].str.count('!').gt(5).sum()}\")\n",
    "print(f\"Reviews with excessive capital letters: {df['review_text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0).gt(0.3).sum()}\")\n",
    "\n",
    "# Sentiment-rating alignment check\n",
    "print(f\"\\nSentiment-Rating Alignment:\")\n",
    "aligned = 0\n",
    "misaligned = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    rating = row['rating']\n",
    "    sentiment = row['vader_sentiment']\n",
    "    \n",
    "    if (rating >= 4 and sentiment == 'positive') or (rating <= 2 and sentiment == 'negative') or (rating == 3 and sentiment == 'neutral'):\n",
    "        aligned += 1\n",
    "    else:\n",
    "        misaligned += 1\n",
    "\n",
    "alignment_rate = aligned / (aligned + misaligned) * 100\n",
    "print(f\"Sentiment-rating alignment rate: {alignment_rate:.1f}%\")\n",
    "print(f\"Aligned reviews: {aligned}\")\n",
    "print(f\"Misaligned reviews: {misaligned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Insights and Recommendations for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights and recommendations\n",
    "print(\"KEY INSIGHTS AND RECOMMENDATIONS FOR MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. DATASET CHARACTERISTICS:\")\n",
    "print(f\"   • Total reviews: {len(df):,}\")\n",
    "print(f\"   • Average review length: {df['word_count'].mean():.0f} words\")\n",
    "print(f\"   • Vocabulary richness: {len(word_freq):,} unique words\")\n",
    "print(f\"   • Rating distribution: Skewed towards positive (60.6% are 4-5 stars)\")\n",
    "\n",
    "print(\"\\n2. SENTIMENT ANALYSIS INSIGHTS:\")\n",
    "print(f\"   • VADER positive sentiment: {(df['vader_sentiment'] == 'positive').mean():.1%}\")\n",
    "print(f\"   • VADER-rating correlation: {df['rating'].corr(df['vader_compound']):.3f}\")\n",
    "print(f\"   • Sentiment-rating alignment: {alignment_rate:.1f}%\")\n",
    "print(f\"   • Misaligned reviews provide valuable training signal\")\n",
    "\n",
    "print(\"\\n3. DATA QUALITY:\")\n",
    "print(f\"   • {duplicate_reviews} duplicate reviews - minimal impact\")\n",
    "print(f\"   • {very_short} very short reviews - consider filtering\")\n",
    "print(f\"   • {empty_text} empty reviews - needs handling\")\n",
    "print(f\"   • Text preprocessing will improve model performance\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDATIONS FOR LSTM MODEL:\")\n",
    "print(\"   • Vocabulary size: Use top 10,000 words to balance coverage and complexity\")\n",
    "print(\"   • Sequence length: 200 tokens should capture ~95% of reviews\")\n",
    "print(\"   • Class imbalance: Consider stratified sampling and class weights\")\n",
    "print(\"   • Use bidirectional LSTM to capture context from both directions\")\n",
    "print(\"   • Apply dropout for regularization due to moderate dataset size\")\n",
    "\n",
    "print(\"\\n5. TRAINING STRATEGY:\")\n",
    "print(\"   • Remove 3-star reviews for clearer binary classification\")\n",
    "print(\"   • Use 80-20 train-test split with stratification\")\n",
    "print(\"   • Monitor both LSTM and VADER performance for comparison\")\n",
    "print(\"   • Focus on misaligned cases for business insights\")\n",
    "\n",
    "print(\"\\n6. EVALUATION METRICS:\")\n",
    "print(\"   • Primary: Accuracy, Precision, Recall, F1-score\")\n",
    "print(\"   • Secondary: Correlation with VADER scores\")\n",
    "print(\"   • Business: Misclassification analysis for actionable insights\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Ready for LSTM model training!\")\n",
    "print(\"Proceed to notebook 02_lstm_training.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset with VADER scores for model training\n",
    "output_file = '../data/processed_reviews_with_vader.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Processed dataset saved to: {output_file}\")\n",
    "print(f\"Dataset includes VADER sentiment scores and cleaned text for model training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}